[
    {
        "id": "AQ1082",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the highest benchmark result achieved on the NYT29 dataset, including the metric and its value?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"NYT29\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1163",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark result (metric and value) over the dataset ARC (Challenge)?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"ARC (Challenge)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1723",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of Score metric on the Atari 2600 Tennis benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Tennis\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1531",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of Test perplexity metric on the WikiText-103 benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Test perplexity\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"WikiText-103\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0543",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of research paper titles and IDs that have benchmarked models on the Cart Pole (OpenAI Gym) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym)\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2060",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the BERTwwm + SQuAD 2 model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"BERTwwm + SQuAD 2\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "HQ0058",
        "query_type": "Non-factoid",
        "question": {
            "string": "What are the precision, recall, and f1 values of all compared studies that used the algorithm naive bayes in combination with the machine learning feature bag of words to classfy user feedback as feature request? "
        },
        "paraphrased_question": [
            "What are the values of precision, recall and f1 metrics of models using naive bayes and bag of words algorithms for user feedback classification?"
        ],
        "query": {
            "sparql": "SELECT ?precision ?recall ?f1\nWHERE {\n  orkgr:R112387 orkgp:compareContribution ?contrib.\n  ?contrib orkgp:P15006 ?algorithms.\n  ?algorithms rdfs:label ?alg_names.\n  FILTER(REGEX(?alg_names, \"Naive bayes\"))\n  ?algorithms orkgp:P36075 ?features.\n  ?features rdfs:label ?fea_names.\n  FILTER(REGEX(?fea_names, \"Bag of words\"))\n  ?features orkgp:P37029 ?categories.\n  ?categories rdfs:label ?cat_names.\n  FILTER(REGEX(?cat_names, \"Feature request\"))\n  ?categories orkgp:P3004 ?precision;\n              orkgp:P5015 ?recall;\n              orkgp:P18037 ?f1.\n}"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 10
    },
    {
        "id": "AQ1904",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of benchmarked datasets related to the Scientific Results Extraction research area?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?dataset ?dataset_lbl\nWHERE {\n  ?problem       a                orkgc:Problem;\n                 rdfs:label       ?problem_lbl.  \n  FILTER (str(?problem_lbl) = \"Scientific Results Extraction\")\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark;\n                  orkgp:P32                ?problem.\n}"
        },
        "template_id": "T06",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2167",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the LSTM (Bai et al., 2018) model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"LSTM (Bai et al., 2018)\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0250",
        "query_type": "Factoid",
        "question": {
            "string": "Could you provide a list of models that have been tested on the Atari 2600 River Raid benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 River Raid\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1812",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest Percentage error score on the SVHN benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Percentage error\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"SVHN\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1985",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the BART model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"BART\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0955",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the Atari 2600 Defender dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Defender\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2087",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the Multi-Perspective Matching (ensemble) model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Multi-Perspective Matching (ensemble)\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1168",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the highest benchmark result achieved on the Supervised: dataset, including the metric and its value?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Supervised:\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1654",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of PARAMS metric on the FGVC Aircraft benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"PARAMS\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"FGVC Aircraft\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ2130",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the Large mLSTM model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Large mLSTM\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0503",
        "query_type": "Factoid",
        "question": {
            "string": "Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the BIOSSES dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"BIOSSES\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1287",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark result (metric and value) over the dataset Atari 2600 Enduro?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Enduro\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0217",
        "query_type": "Factoid",
        "question": {
            "string": "Could you provide a list of models that have been tested on the Reuters RCV1/RCV2 English-to-German benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Reuters RCV1/RCV2 English-to-German\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "HQ0027",
        "query_type": "Factoid/Superlative",
        "question": {
            "string": "For which country of study overall prevalence of epilepsy is the highest?"
        },
        "paraphrased_question": [
            "In which country is the overall prevalence of epilepsy the highest?"
        ],
        "query": {
            "sparql": "SELECT ?country, ?country_label\nWHERE {\n  orkgr:R75729 orkgp:compareContribution ?contrib.\n  ?contrib orkgp:P15512 ?country.\n  ?contrib orkgp:P16013 ?overall_prevalence.\n  ?country rdfs:label ?country_label.\n  ?overall_prevalence rdfs:label ?overall_prevalence_value\n}\nORDER BY DESC(?overall_prevalence_value)\nLIMIT 1"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ0952",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Up and Down dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Up and Down\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0742",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the DuIE dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"DuIE\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1578",
        "query_type": "Factoid",
        "question": {
            "string": "What is the best performing model benchmarking the X-Sum dataset in terms of ROUGE-2 metric?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"ROUGE-2\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"X-Sum\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0299",
        "query_type": "Factoid",
        "question": {
            "string": "What models are being evaluated on the Classic dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Classic\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0835",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the enwik8 benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"enwik8\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1422",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of BLEU score metric on the IWSLT2014 German-English benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"BLEU score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"IWSLT2014 German-English\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1571",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of F1 entity level score when benchmarked on the NCBI Disease dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"F1 entity level\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"NCBI Disease\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1425",
        "query_type": "Factoid",
        "question": {
            "string": "What is the best performing model benchmarking the WMT2014 German-English dataset in terms of BLEU metric?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"BLEU\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"WMT2014 German-English\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0829",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the CommitmentBank dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"CommitmentBank\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2462",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list benchmarked problems in the area of Artificial Intelligence?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?problem ?problem_lbl\nWHERE {\n  ?rf       a            orkgc:ResearchField;\n            rdfs:label   ?rf_label.\n  FILTER (str(?rf_label) = \"Artificial Intelligence\")\n  ?paper    orkgp:P30    ?rf;\n            orkgp:P31    ?cont.\n  ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                orkgp:P32                ?problem.\n  ?problem      rdfs:label               ?problem_lbl.\n}"
        },
        "template_id": "T08",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ0607",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Bank Heist dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Bank Heist\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1376",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest F1 score score on the Penn Treebank benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"F1 score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Penn Treebank\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ2328",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the MFEC model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"MFEC\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0813",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the SQuAD2.0 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SQuAD2.0\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0963",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Bowling dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Bowling\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0026",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the models that have been evaluated on the SciTLDR dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SciTLDR\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1447",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of Pre-Training Dataset metric on the HMDB51 benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Pre-Training Dataset\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"HMDB51\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0653",
        "query_type": "Factoid",
        "question": {
            "string": "Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the SciCite dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SciCite\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ0721",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the DRI Corpus dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"DRI Corpus\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2293",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the Duel noop model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Duel noop\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ2287",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the Pointer + Coverage + EntailmentGen + QuestionGen model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Pointer + Coverage + EntailmentGen + QuestionGen\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0991",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the Reuters De-En dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Reuters De-En\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0914",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the metrics used to evaluate models on the Barabasi-Albert dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Barabasi-Albert\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0157",
        "query_type": "Factoid",
        "question": {
            "string": "What models are being evaluated on the Penn Treebank (Character Level) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2397",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the DeiT-Ti model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"DeiT-Ti\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0964",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the Atari 2600 Battle Zone benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1361",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the ModelNet40 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"ModelNet40\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ2142",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the 12-layer Transformer-XL model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"12-layer Transformer-XL\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1860",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of benchmarked datasets related to the Reading Comprehension research area?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?dataset ?dataset_lbl\nWHERE {\n  ?problem       a                orkgc:Problem;\n                 rdfs:label       ?problem_lbl.  \n  FILTER (str(?problem_lbl) = \"Reading Comprehension\")\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark;\n                  orkgp:P32                ?problem.\n}"
        },
        "template_id": "T06",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2380",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the Tsetlin Machine model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Tsetlin Machine\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ2273",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the AcrE model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"AcrE\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0727",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the FSNS - Test dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"FSNS - Test\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1221",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Ball in cup, catch (DMControl500k)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0570",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the WSC dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"WSC\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2068",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the DCN model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"DCN\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1873",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of benchmarked datasets related to the Audio Classification research area?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?dataset ?dataset_lbl\nWHERE {\n  ?problem       a                orkgc:Problem;\n                 rdfs:label       ?problem_lbl.  \n  FILTER (str(?problem_lbl) = \"Audio Classification\")\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark;\n                  orkgp:P32                ?problem.\n}"
        },
        "template_id": "T06",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1807",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Kuzushiji-MNIST\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0798",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the MultiRC dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"MultiRC\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1044",
        "query_type": "non-factoid",
        "question": {
            "string": "Can you provide the highest benchmark result, including the metric and score, for the Softcite dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Softcite\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1678",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of Accuracy score when benchmarked on the MLDoc Zero-Shot English-to-French dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-French\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1787",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest Score score on the Atari 2600 Yars Revenge benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Yars Revenge\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0726",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the seel.cse.lsu.edu/data/re17.zip  dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1854",
        "query_type": "Factoid",
        "question": {
            "string": "What are the most commonly used benchmark datasets for the Joint Entity and Relation Extraction research field?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?dataset ?dataset_lbl\nWHERE {\n  ?problem       a                orkgc:Problem;\n                 rdfs:label       ?problem_lbl.  \n  FILTER (str(?problem_lbl) = \"Joint Entity and Relation Extraction\")\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark;\n                  orkgp:P32                ?problem.\n}"
        },
        "template_id": "T06",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1742",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of Unpermuted Accuracy score when benchmarked on the Sequential CIFAR-10 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Unpermuted Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Sequential CIFAR-10\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1977",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the STREET model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"STREET\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0837",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the Hutter Prize dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Hutter Prize\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1180",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the Hutter Prize dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Hutter Prize\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1087",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark result (metric and value) over the dataset ACE 2004?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"ACE 2004\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0278",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the models that have been evaluated on the Atari 2600 Battle Zone dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Battle Zone\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "HQ0078",
        "query_type": "Non-factoid/Superlative",
        "question": {
            "string": "Where did the study with maximal geographic scale take place?"
        },
        "paraphrased_question": [
            "What is the studied location with the largest geographic scale?"
        ],
        "query": {
            "sparql": "SELECT ?location, ?location_label\nWHERE {\n  {\n    SELECT (MAX(?geo_scale) AS ?max_geo_scale)\n    WHERE {\n      orkgr:R149849 orkgp:compareContribution ?contrib.\n      ?contrib orkgp:P41568 ?geo_scale.\n    }\n  }\n  orkgr:R149849 orkgp:compareContribution ?contrib.\n  ?contrib orkgp:P41568 ?geo_scale;\n           orkgp:P37524 ?location.\n  ?location rdfs:label ?location_label.\n  FILTER(?geo_scale = ?max_geo_scale)\n}"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "tree",
        "query_class": "WHICH-WHERE",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1113",
        "query_type": "non-factoid",
        "question": {
            "string": "Can you provide the highest benchmark result, including the metric and score, for the IWSLT2015 German-English dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"IWSLT2015 German-English\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1649",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest SUCCESS score on the Habitat 2020 Object Nav test-std benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"SUCCESS\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Habitat 2020 Object Nav test-std\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1370",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the Nottingham dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Nottingham\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0978",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the DBpedia benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"DBpedia\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1743",
        "query_type": "Factoid",
        "question": {
            "string": "What is the best performing model benchmarking the BUCC German-to-English dataset in terms of F1 score metric?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"F1 score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"BUCC German-to-English\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0807",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the Story Cloze Test benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Story Cloze Test\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2387",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the BiT-M (ResNet) model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"BiT-M (ResNet)\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0666",
        "query_type": "Factoid",
        "question": {
            "string": "Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the ObjectNet dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ObjectNet\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1484",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of F1 score when benchmarked on the Natural Questions (long) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"F1\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1251",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark result (metric and value) over the dataset MLDoc Zero-Shot English-to-Spanish?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"MLDoc Zero-Shot English-to-Spanish\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0435",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the ImageNet 64x64 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ImageNet 64x64\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1300",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the Atari 2600 Ice Hockey dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Ice Hockey\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1430",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest BLEU score score on the WMT2014 English-German benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"BLEU score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"WMT2014 English-German\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1216",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the BC5CDR-disease dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0298",
        "query_type": "Factoid",
        "question": {
            "string": "Could you provide a list of models that have been tested on the IMDb-M benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"IMDb-M\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0097",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the models that have been evaluated on the Multimodal PISA dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Multimodal PISA\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2304",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the Rational DQN Average model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Rational DQN Average\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0128",
        "query_type": "Factoid",
        "question": {
            "string": "What are the models that have been benchmarked on the SearchQA dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SearchQA\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0792",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the Quasart-T benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Quasart-T\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2342",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the DQNMMCe+SR model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"DQNMMCe+SR\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0168",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the models that have been evaluated on the STS Benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"STS Benchmark\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0958",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the Atari 2600 Tutankham dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Tutankham\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0164",
        "query_type": "Factoid",
        "question": {
            "string": "What models are being evaluated on the TempEval-3 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"TempEval-3\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ2203",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the 6-layer QRNN model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"6-layer QRNN\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0946",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the Atari 2600 Fishing Derby benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Fishing Derby\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1119",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the Stanford Dogs dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Stanford Dogs\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0007",
        "query_type": "Factoid",
        "question": {
            "string": "What models are being evaluated on the ACL Anthology dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ACL Anthology\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "HQ0057",
        "query_type": "Non-factoid",
        "question": {
            "string": "Where can all the data sets used in the compared studies be found?"
        },
        "paraphrased_question": [
            "What are URLs for the considered studies?"
        ],
        "query": {
            "sparql": "SELECT DISTINCT ?URL\nWHERE {\n  orkgr:R112387 orkgp:compareContribution ?contrib.\n  ?contrib orkgp:HAS_DATASET ?URL.\n  FILTER(!REGEX(?URL, \"Not\"))\n}"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "chain",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 2
    },
    {
        "id": "AQ1568",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest F1 score on the CoNLL 2003 (English) benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"F1\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"CoNLL 2003 (English)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0059",
        "query_type": "Factoid",
        "question": {
            "string": "What are the models that have been benchmarked on the ACE 2005 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ACE 2005\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1272",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the Atari 2600 Breakout dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Breakout\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ2249",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the SemExp model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"SemExp\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1475",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Story Cloze Test\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0439",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of research paper titles and IDs that have benchmarked models on the Kinetics-600 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Kinetics-600\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1013",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the Sequential CIFAR-10 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Sequential CIFAR-10\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0190",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the models that have been evaluated on the ShARe/CLEF eHealth corpus dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ShARe/CLEF eHealth corpus\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1369",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark result (metric and value) over the dataset IMDb-B?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"IMDb-B\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0348",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the SciREX dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SciREX\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "HQ0081",
        "query_type": "Factoid",
        "question": {
            "string": "What quantity of iron oxide was discovered on Elorza crater?"
        },
        "paraphrased_question": [
            "What amount of iron oxide was detected on Elorza crater on Mars?"
        ],
        "query": {
            "sparql": "SELECT ?properties_values, ?property_description\nWHERE {\n  ?papers rdf:type orkgc:Paper.\n  ?papers rdfs:label ?papers_labels.\n  FILTER(REGEX(?papers_labels, \"Elorza crater\", \"i\"))\n  ?papers orkgp:P31 ?contrib.\n  ?contrib ?properties ?properties_values.\n  ?properties rdfs:label ?properties_labels.\n  FILTER(REGEX(?properties_labels, \"FeO\"))\n  ?properties orkgp:description ?property_description.\n}"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1953",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the CL-Titles-Parser model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"CL-Titles-Parser\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0039",
        "query_type": "Factoid",
        "question": {
            "string": "Could you provide a list of models that have been tested on the seel.cse.lsu.edu/data/refsq17.zip benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/refsq17.zip\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0645",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the HoC dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"HoC\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2094",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the OTF spelling+lemma (single) model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"OTF spelling+lemma (single)\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1710",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of Score metric on the Atari 2600 Freeway benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Freeway\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1391",
        "query_type": "Factoid",
        "question": {
            "string": "What is the best performing model benchmarking the ACE 2004 dataset in terms of RE+ Micro F1 metric?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"RE+ Micro F1\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"ACE 2004\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0611",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the Atari 2600 Asterix dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Asterix\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2033",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the Unsupervised NMT + weight-sharing model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Unsupervised NMT + weight-sharing\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1029",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the metrics used to evaluate models on the DocRED (Human-annotated) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"DocRED (Human-annotated)\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1492",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of Precision metric on the RotoWire (Relation Generation) benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Precision\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"RotoWire (Relation Generation)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1839",
        "query_type": "Factoid",
        "question": {
            "string": "Name the datasets that have been used for benchmarking in the Image Classification research problem?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?dataset ?dataset_lbl\nWHERE {\n  ?problem       a                orkgc:Problem;\n                 rdfs:label       ?problem_lbl.  \n  FILTER (str(?problem_lbl) = \"Image Classification\")\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark;\n                  orkgp:P32                ?problem.\n}"
        },
        "template_id": "T06",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1103",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the highest benchmark result achieved on the WMT2016 English-Russian dataset, including the metric and its value?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"WMT2016 English-Russian\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0888",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the Cheetah, run (DMControl500k) benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Cheetah, run (DMControl500k)\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "HQ0038",
        "query_type": "Non-factoid/Count",
        "question": {
            "string": "What is the mean capacity of a carbon-based fuel?"
        },
        "paraphrased_question": [
            "What is the value of average capacity for carbon fuel?"
        ],
        "query": {
            "sparql": "SELECT (AVG(?numerical_capacity) AS ?mean) \nWHERE {\n  ?paper a orkgc:Paper;\n         orkgp:P31 [\n           orkgp:P15483 ?capacity\n         ];\n         rdfs:label ?title.\n  BIND(\n    xsd:double(\n      REPLACE(\n        STR(\n          REPLACE(\n            ?capacity,\n            \"([0-9]+),([0-9]+)\",\n            \"$1$2\"\n          )\n        ),\n        \"([0-9]+).*\",\n        \"$1\"\n      ) \n    ) AS ?numerical_capacity\n  )\n  FILTER(REGEX(STR(?title), \"(fuel|CO2)\"))\n}"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1402",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of F1 score when benchmarked on the NYT-single dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"F1\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"NYT-single\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1433",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of BLEU score metric on the WMT2016 English-German benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"BLEU score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"WMT2016 English-German\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1639",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest Score score on the Cheetah, run (DMControl500k) benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Cheetah, run (DMControl500k)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1537",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of Number of params score when benchmarked on the Penn Treebank (Character Level) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Number of params\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Penn Treebank (Character Level)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0863",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the SST-5 Fine-grained classification dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SST-5 Fine-grained classification\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0713",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the SciGEN dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SciGEN\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0452",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the PubMedQA dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"PubMedQA\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1806",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest Top 1 Accuracy score on the ImageNet V2 benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Top 1 Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"ImageNet V2\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1994",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the HRLRE model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"HRLRE\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1171",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the MultiNLI dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"MultiNLI\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1498",
        "query_type": "Factoid",
        "question": {
            "string": "What is the best performing model benchmarking the PIQA dataset in terms of Accuracy metric?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"PIQA\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1229",
        "query_type": "non-factoid",
        "question": {
            "string": "Can you provide the highest benchmark result, including the metric and score, for the Cart Pole (OpenAI Gym) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Cart Pole (OpenAI Gym)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0410",
        "query_type": "Factoid",
        "question": {
            "string": "List the title and ID of research papers that contain a benchmark over the NYT24 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"NYT24\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ0652",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the PubMed 20k RCT dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"PubMed 20k RCT\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ1722",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of Score score when benchmarked on the Atari 2600 Tutankham dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Tutankham\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0573",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the arXiv dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"arXiv\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2355",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the BiLSTM-Attention + ELMo model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"BiLSTM-Attention + ELMo\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1976",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the SEE model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"SEE\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0085",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the models that have been evaluated on the WMT2016 English-German dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"WMT2016 English-German\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1275",
        "query_type": "non-factoid",
        "question": {
            "string": "Can you provide the highest benchmark result, including the metric and score, for the Atari 2600 Berzerk dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0786",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the UCF101 (finetuned) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"UCF101 (finetuned)\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0932",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the Atari 2600 Berzerk dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Berzerk\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1945",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the H-NLI model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"H-NLI\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0897",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the Oxford-IIIT Pets benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Oxford-IIIT Pets\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1017",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the metrics used to evaluate models on the BUCC Russian-to-English dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"BUCC Russian-to-English\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1603",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of FLOPS metric on the CIFAR-100 benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"FLOPS\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"CIFAR-100\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ2385",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the EfficientNetV2-L model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"EfficientNetV2-L\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1160",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark result (metric and value) over the dataset RotoWire (Relation Generation)?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"RotoWire (Relation Generation)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "HQ0032",
        "query_type": "Factoid/Superlative",
        "question": {
            "string": "What is the most common lead compound?"
        },
        "paraphrased_question": [
            "Which is the mostly used lead compound?"
        ],
        "query": {
            "sparql": "SELECT ?compound\nWHERE {\n  orkgr:R75638 orkgp:compareContribution ?contrib.\n  ?contrib orkgp:P35194 ?compound.\n}\nORDER BY DESC(COUNT(?compound))\nLIMIT 1"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "chain",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 2
    },
    {
        "id": "AQ1631",
        "query_type": "Factoid",
        "question": {
            "string": "What is the best performing model benchmarking the Reacher, easy (DMControl100k) dataset in terms of Score metric?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Reacher, easy (DMControl100k)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0107",
        "query_type": "Factoid",
        "question": {
            "string": "What are the models that have been benchmarked on the Natural Questions (short) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Natural Questions (short)\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1931",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the Switch Transformer model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Switch Transformer\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1197",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the highest benchmark result achieved on the STS Benchmark dataset, including the metric and its value?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"STS Benchmark\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ0301",
        "query_type": "Factoid",
        "question": {
            "string": "What models are being evaluated on the WOS-11967 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"WOS-11967\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1920",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the Funnel Transformer model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"Funnel Transformer\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1472",
        "query_type": "Factoid",
        "question": {
            "string": "What is the name of the top performing model in terms of Accuracy score when benchmarked on the TriviaQA dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"TriviaQA\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0779",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the Fashion-MNIST dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Fashion-MNIST\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1733",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest Score score on the Atari 2600 Q*Bert benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Q*Bert\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1001",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the iNaturalist 2019 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"iNaturalist 2019\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1069",
        "query_type": "non-factoid",
        "question": {
            "string": "Can you provide the highest benchmark result, including the metric and score, for the seel.cse.lsu.edu/data/re17.zip  dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"seel.cse.lsu.edu/data/re17.zip \")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1155",
        "query_type": "non-factoid",
        "question": {
            "string": "Can you provide the highest benchmark result, including the metric and score, for the Natural Questions (long) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Natural Questions (long)\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1813",
        "query_type": "Factoid",
        "question": {
            "string": "What is the best performing model benchmarking the iNaturalist 2018 dataset in terms of Top-1 Accuracy metric?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Top-1 Accuracy\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"iNaturalist 2018\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ0182",
        "query_type": "Factoid",
        "question": {
            "string": "What models are being evaluated on the NCBI-disease dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"NCBI-disease\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1169",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the Words in Context dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Words in Context\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ2205",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the NASCell model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"NASCell\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0777",
        "query_type": "Factoid",
        "question": {
            "string": "What are the metrics of evaluation over the Stanford Cars dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Stanford Cars\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1692",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of Score metric on the Atari 2600 Crazy Climber benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Score\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Crazy Climber\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1144",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark result (metric and value) over the dataset CommonsenseQA?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"CommonsenseQA\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1317",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the highest benchmark result achieved on the Atari 2600 Star Gunner dataset, including the metric and its value?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Star Gunner\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1405",
        "query_type": "Factoid",
        "question": {
            "string": "Which model has achieved the highest Entity F1 score on the SciERC benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"Entity F1\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"SciERC\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ1286",
        "query_type": "non-factoid",
        "question": {
            "string": "What is the top benchmark score and its metric on the Atari 2600 Alien dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Atari 2600 Alien\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "AQ1073",
        "query_type": "non-factoid",
        "question": {
            "string": "Can you provide the highest benchmark result, including the metric and score, for the Pubmed dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl (MAX(?value) AS ?score)\nWHERE {\n  {\n    SELECT ?metric ?metric_lbl ?value\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"Pubmed\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value.\n      OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n               ?metric          rdfs:label               ?metric_lbl.}\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark.\n      OPTIONAL {?cont      orkgp:HAS_MODEL          ?model.\n                ?model      rdfs:label               ?model_lbl.}\n    }\n    ORDER BY DESC(?value)\n  }\n}\nGROUP BY ?metric ?metric_lbl"
        },
        "template_id": "T04",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 13
    },
    {
        "id": "HQ0091",
        "query_type": "Factoid",
        "question": {
            "string": "What is Raman spectroscopy?"
        },
        "paraphrased_question": [
            "What is the definition of Raman spectroscopy?"
        ],
        "query": {
            "sparql": "SELECT ?definitions\nWHERE {\n  ?terms orkgp:P24009 ?definitions.\n  ?terms rdfs:label ?terms_labels.\n  FILTER(REGEX(?terms_labels, \"Raman spectroscopy\"))\n}"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "star",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 2
    },
    {
        "id": "HQ0022",
        "query_type": "Factoid",
        "question": {
            "string": "Does paper \"Disambiguating authors in citations on the web and authorship correlations\" employe Cosine similarity?"
        },
        "paraphrased_question": [
            "Are contributions from the paper \"Disambiguating authors in citations on the web and authorship correlations\" based on cosine similarity?"
        ],
        "query": {
            "sparql": "ASK\nWHERE {\n  orkgr:R6187 orkgp:compareContribution ?cont.\n  ?paper orkgp:P31 ?cont;\n         rdfs:label ?paper_title.\n  ?cont orkgp:P5002 orkgr:R6006.\n  FILTER(REGEX(STR(?paper_title), \"Disambiguating authors in citations on the web and authorship correlations\", \"i\"))\n}"
        },
        "template_id": null,
        "auto_generated": false,
        "query_shape": "tree",
        "query_class": "BOOLEAN",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0537",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the Reacher, easy (DMControl100k) dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Reacher, easy (DMControl100k)\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2077",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the MEMEN model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"MEMEN\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0409",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of research paper titles and IDs that have benchmarked models on the WLPC dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"WLPC\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ0282",
        "query_type": "Factoid",
        "question": {
            "string": "What are the models that have been benchmarked on the Atari 2600 Space Invaders dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Space Invaders\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0850",
        "query_type": "Factoid",
        "question": {
            "string": "What evaluation metrics are commonly used when benchmarking models on the TempEval-3 dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"TempEval-3\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0023",
        "query_type": "Factoid",
        "question": {
            "string": "Could you provide a list of models that have been tested on the SciCite benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"SciCite\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0209",
        "query_type": "Factoid",
        "question": {
            "string": "Can you list the models that have been evaluated on the ClueWeb09-B dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ClueWeb09-B\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ1587",
        "query_type": "Factoid",
        "question": {
            "string": "Indicate the model that performed best in terms of F1 metric on the PubMed 20k RCT benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?metric     a       orkgc:Metric;\n              rdfs:label  ?metric_lbl.\n  FILTER (str(?metric_lbl) = \"F1\")\n  {\n    SELECT ?model ?model_lbl\n    WHERE {\n      ?dataset       a                orkgc:Dataset;\n                      rdfs:label       ?dataset_lbl.\n      FILTER (str(?dataset_lbl) = \"PubMed 20k RCT\")\n      ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                      orkgp:HAS_EVALUATION    ?eval.\n      ?eval           orkgp:HAS_VALUE         ?value;\n                      orkgp:HAS_METRIC         ?metric.\n      ?cont         orkgp:HAS_BENCHMARK      ?benchmark;\n                    orkgp:HAS_MODEL          ?model.\n      ?model      rdfs:label               ?model_lbl.\n    }\n    ORDER BY DESC(?value)\n    LIMIT 1\n  }\n}"
        },
        "template_id": "T05",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 12
    },
    {
        "id": "AQ2309",
        "query_type": "Factoid",
        "question": {
            "string": "List the code links in papers that use the A3C FF (1 day) hs model in any benchmark?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"A3C FF (1 day) hs\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ2426",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of papers that have utilized the DY-MobileNetV3-Small model and include the links to their code?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"DY-MobileNetV3-Small\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0530",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of research paper titles and IDs that have benchmarked models on the BC5CDR-disease dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"BC5CDR-disease\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ0639",
        "query_type": "Factoid",
        "question": {
            "string": "List the title and ID of research papers that contain a benchmark over the Ohsumed dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Ohsumed\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2176",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the PAR Transformer Large model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"PAR Transformer Large\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0717",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the ART/CoreSC benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ART/CoreSC\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0259",
        "query_type": "Factoid",
        "question": {
            "string": "What models are being evaluated on the Atari 2600 Solaris dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?model ?model_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Atari 2600 Solaris\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  ?paper         orkgp:HAS_BENCHMARK      ?benchmark.\n  OPTIONAL {?paper      orkgp:HAS_MODEL          ?model.\n            ?model      rdfs:label               ?model_lbl.}\n}"
        },
        "template_id": "T01",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0517",
        "query_type": "Factoid",
        "question": {
            "string": "What are the titles and IDs of research papers that include a benchmark for the DCASE dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"DCASE\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2046",
        "query_type": "Factoid",
        "question": {
            "string": "Where can I find code references in papers that have used the MMV model for benchmarking purposes?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"MMV\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ1905",
        "query_type": "Factoid",
        "question": {
            "string": "List the datasets benchmarked under the SPARQL query optimization research problem?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?dataset ?dataset_lbl\nWHERE {\n  ?problem       a                orkgc:Problem;\n                 rdfs:label       ?problem_lbl.  \n  FILTER (str(?problem_lbl) = \"SPARQL query optimization\")\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark;\n                  orkgp:P32                ?problem.\n}"
        },
        "template_id": "T06",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ0735",
        "query_type": "Factoid",
        "question": {
            "string": "List the metrics that are used to evaluate models on the Penn Treebank benchmark dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?metric ?metric_lbl\nWHERE {\n  ?dataset       a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"Penn Treebank\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset;\n                  orkgp:HAS_EVALUATION    ?eval.\n  OPTIONAL {?eval           orkgp:HAS_METRIC         ?metric.\n           ?metric          rdfs:label               ?metric_lbl.}\n}"
        },
        "template_id": "T03",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 6
    },
    {
        "id": "AQ0661",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of research paper titles and IDs that have benchmarked models on the ImageNet ReaL dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"ImageNet ReaL\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    },
    {
        "id": "AQ2083",
        "query_type": "Factoid",
        "question": {
            "string": "Can you provide links to code used in papers that benchmark the MEMEN (single model) model?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?code\nWHERE {\n  ?model    a            orkgc:Model;\n            rdfs:label    ?model_lbl.\n  FILTER (str(?model_lbl) = \"MEMEN (single model)\")\n  ?benchmark      orkgp:HAS_DATASET        ?dataset.\n  ?cont           orkgp:HAS_BENCHMARK      ?benchmark.\n  ?cont           orkgp:HAS_MODEL          ?model;\n                  orkgp:HAS_SOURCE_CODE    ?code.\n}"
        },
        "template_id": "T07",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 4
    },
    {
        "id": "AQ0422",
        "query_type": "Factoid",
        "question": {
            "string": "Provide a list of research paper titles and IDs that have benchmarked models on the WMT2016 English-Romanian dataset?"
        },
        "paraphrased_question": [],
        "query": {
            "sparql": "SELECT DISTINCT ?paper ?paper_lbl\nWHERE {\n  ?dataset        a                orkgc:Dataset;\n                  rdfs:label       ?dataset_lbl.\n  FILTER (str(?dataset_lbl) = \"WMT2016 English-Romanian\")\n  ?benchmark      orkgp:HAS_DATASET       ?dataset.\n  ?cont          orkgp:HAS_BENCHMARK      ?benchmark.\n  ?paper         orkgp:P31                ?cont;\n                 rdfs:label               ?paper_lbl.\n}"
        },
        "template_id": "T02",
        "auto_generated": true,
        "query_shape": "Tree",
        "query_class": "WHICH-WHAT",
        "number_of_patterns": 5
    }
]